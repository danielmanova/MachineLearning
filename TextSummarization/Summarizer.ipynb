{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8266650-bc4a-4b5f-b16e-6cb80225d1a6",
   "metadata": {},
   "source": [
    "<b>Name:</b> Daniel Manova<br>\n",
    "<b>Project:</b> Text Summarization using NLTK<br>\n",
    "\n",
    "<b>Problem Statement:</b> Text Summarization for the. give file of type .txt,.docx or web url using NLP based technique<br>\n",
    "<b>Objective:</b> Summarize the text using nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ba4bc-030e-44a6-b8a5-793b5740b3fa",
   "metadata": {},
   "source": [
    "## Input the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7fd3e2-1347-4aac-8af5-9d4cb4930ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all necessary packages\n",
    "import nltk #NLTK is a libraries for Natural Language Processing. It is a platform for building Python programs to process natural language. NLTK is written in the Python programming language.\n",
    "import re #Built-in package to work with Regular Expressions.\n",
    "import bs4 as bs #Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "import urllib.request  #The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP) in a complex world â€” basic and digest authentication, redirections, cookies and more.0.\n",
    "import docx2txt #library used to convert Microsoft Office Docx documents to equivalent Text documents.\n",
    "import heapq #Heap queue is a special tree structure in which each parent node is less than or equal to its child node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1fd2c-ac66-4441-873b-25b6aca57e2a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff2b99-1be3-4fdf-aca1-7a08ac8f6afc",
   "metadata": {},
   "source": [
    "#### Reading text Content\n",
    "- We can summarise the content which are of file type  MS-Word,text,Weburl\n",
    "- Library docx2txt is used to read the content from word document and convert to text format\n",
    "- Module urllib.request to open the web url and with help of Beautiful Soup pull data out of HTML and XML files.\n",
    "- open method is used to read text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b21996-7276-4918-b709-7fa0eb7ac8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_content(filepath_or_url,filetype):\n",
    "    if filetype == 'word':\n",
    "        doc_content = docx2txt.process(filepath_or_url)\n",
    "    elif filetype == 'url':\n",
    "        scraped_data = urllib.request.urlopen(filepath_or_url)\n",
    "        content = scraped_data.read()\n",
    "        parsed_content = bs.BeautifulSoup(content,'lxml')\n",
    "        paragraphs = parsed_content.find_all('p')\n",
    "        doc_content = \"\"\n",
    "        for p in paragraphs:\n",
    "            doc_content += p.text       \n",
    "    elif filetype == 'text':\n",
    "        file = open(filepath_or_url, 'r', errors='ignore')\n",
    "        doc_content=file.read()\n",
    "    else:\n",
    "        print(\"Please makesure the file type is .txt, .docx or url\\n\")\n",
    "    return doc_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e48fa-6573-439f-a938-fe01f93b90f1",
   "metadata": {},
   "source": [
    "#### Text cleaning\n",
    "- Remove special characters and extra spaces from the content\n",
    "- Generate the list of sentences from the content\n",
    "- Define a variable stopwords and store all the English stop words from the nltk library which would be used for checking stop words in the given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3b48f0-6006-4172-a80a-5b7bd1ed1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(data_to_clean):\n",
    "    # Removing some special characers and extra space\n",
    "    data_to_clean = re.sub(r'\\[[0-9]*\\]', ' ', data_to_clean)\n",
    "    data_to_clean = re.sub(r'\\s+', ' ', data_to_clean)\n",
    "\n",
    "    # Removing special characters and digits\n",
    "    formated_content = re.sub('[^a-zA-Z]', ' ', data_to_clean )\n",
    "    formated_content = re.sub(r'\\s+', ' ', formated_content)\n",
    "\n",
    "    # Fetch sentence lenth \n",
    "    sentence_list = nltk.sent_tokenize(data_to_clean)\n",
    "\n",
    "    # Set stopwords using nltk corpus\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    return formated_content,sentence_list,stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b401aff-6aee-4666-bc2f-469f868dcdaa",
   "metadata": {},
   "source": [
    "#### Identify frequency of each word by Occurrence\n",
    "\n",
    "- Lets identify the frequency of each words by using formated_content and stopwords variable extracted from cleanup function.\n",
    "- Makeuse of stopwords variable.Loopover all the sentences and than respective words and check if they are stopwords.\n",
    "    - If  present do nothing\n",
    "    - If not, check whether the words present in word_frequency dictionary.\n",
    "        - if the word is comeup for the first time, it will added to the word_frequencies dictionary as a key and its value is set to 1\n",
    "        - If the word previously exists in the dictionary, its value is simply updated by 1.\n",
    "- Now to find the weighted frequency, lets divide the number of occurances of all the words by the frequency of the most occurring word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d61431c-7d29-40fa-bdbf-0dff84e37202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_max_freq(formated_content,stopwords):\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formated_content):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1 \n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "        maximum_frequncy = max(word_frequencies.values())\n",
    "    return maximum_frequncy,word_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b32eb7-663b-475a-be53-5f91dec81d04",
   "metadata": {},
   "source": [
    "#### Generate Scores for the Sentences\n",
    "\n",
    "- Lets calculate the score of each sentence by adding weighted frequencies of the words that occur in that particular sentence.\n",
    "- Create an empty sentence_scores dictionary and update the Sentences as key and the corresponding score would be defined as value, loop through each sentence in the sentence_list and tokenize the sentence into words.\n",
    "- Check if the word exists in the word_frequencies dictionary fetched from word_max_freq function.This validation is performed since we created the sentence_list list from the doc_content object; on the other hand, the word frequencies were calculated using the formated_content object, which do not contain any stop words, numbers,..\n",
    "- Logic defined to calculate scores for the sentences only contains words less than 28 words, as we do not want long content in the summary.This number can been tweeked specific to usecase.\n",
    "- Check if the sentence available in the sentence_scores dictionary. \n",
    "    - If the sentence doesn't exist, we add it to the sentence_scores dictionary as a key and assign it the weighted frequency of the first word in the sentence, as its value.\n",
    "    - If the sentence exists in the dictionary, add the weighted frequency of the word to the existing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b103a8-76e5-46e1-b443-ecda21f184b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sent_score(word_frequencies,maximum_frequncy):\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 28:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf1267-9c7d-41f4-9f74-c46f72bd561a",
   "metadata": {},
   "source": [
    "#### Generate the Summary\n",
    "- Lets make us of sentence_scores dictionary (contains respective sentence score) to summarize the doc_content\n",
    "- making use of nlargest function to generate the top n sentences with the highest scores (In this we have used 5 sentences. However we can modify specific to usecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9184826e-8fdf-4d27-bbfe-d2a89eef84d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the summary of the provided document:\n",
      "---------------------------------------------\n",
      "  Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. A machine with general intelligence can solve a wide variety of problems with a breadth and versatility similar to human intelligence. A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others. [q] AI founder John McCarthy said: \"Artificial intelligence is not, by definition, simulation of human intelligence\".\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "#text_to_clean = read_content(\"about_corona.docx\",\"word\") #Use this for summarizing the word document\n",
    "#text_to_clean = read_content(\"about_climat_change.txt\",\"text\") #use this for summarizing text document\n",
    "text_to_clean = read_content(\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\"url\") #use this for summarizing web url document\n",
    "\n",
    "formated_content,sentence_list,stopwords = cleanup(text_to_clean)\n",
    "maximum_frequncy,word_frequencies = word_max_freq(formated_content,stopwords)\n",
    "sentence_scores = sent_score(word_frequencies,maximum_frequncy)\n",
    "\n",
    "summary_sentences = heapq.nlargest(5, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(\"This is the summary of the provided document:\\n---------------------------------------------\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad86594-a149-41f7-944b-208ed4eed558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
